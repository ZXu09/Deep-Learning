## 残差网络
在深度学习中，网络层数增多一般会伴着下面几个问题
1. 计算资源的消耗->集群GPU
2. 模型容易过拟合->正则化Dropout
3. 梯度消失/梯度爆炸问题的产生->Batch Normalization

### 梯度消失问题
**反向传播**更新参数时用的是**链式法则**，在很深的网络层中，由于**参数初始化一般更靠近0**，这样在训练的过程中更新浅层网络的参数时，很容易使得**梯度就接近于0**，而导致梯度消失，浅层的参数无法更新。

### 网络退化问题
由**冗余的网络层**学习了**不是恒等映射的参数**造成的。  
假设已经有了一个最优化的网络结构，是18层，假设设计了多于18层的网络模型，往往模型很难将这多余层恒等映射的参数学习正确，那么就一定会不比最优化的18层网络结构性能好，这就是随着网络深度增加，模型会产生退化现象。它**不是由过拟合产生的**。

1. 网络发生退化：随着网络层数的增加，网络发生了**退化（degradation）** 的现象：随着网络层数的增多，训练集loss逐渐下降，然后趋于饱和，当你再增加网络深度的话，**训练集loss反而会增大**。注意这并不是过拟合，因为在过拟合中训练loss是一直减小的。
2. 特征传递：当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把**低层的特征传到高层**，那么效果应该至少不比浅层的网络效果差。
3. 具体方法：直接映射。或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的**特征**，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条**直接映射（Identity Mapping）** 来达到此效果。
4. 残差网络：保证信息不丢失。前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的**直接映射**的加入，保证了 l+1 层的网络一定比 l 层包含**更多的图像信息**。基于这种使用**直接映射**来连接网络不同层直接的思想，**残差网络**应运而生。

<div align=center>
<img src="https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/残差网络.jpg"/>
</div>

### 为什么叫残差网络
在统计学中，残差和误差是非常容易混淆的两个概念。误差是衡量**真实值和观测值**之间的差距，残差是指**预测值和观测值**之间的差距。  
网络的一层通常可以看做`y = H(x)`，而残差网络的一个残差块可以表示为`H(x) = F(x) + x`，`F(x)`是残差块  
**也就是`F(x) = H(x) - x`，`H(x)`是观测值，而`x`是观测值**，所以便对应着残差，因此叫做残差网络。

### 直接映射是最好的选择
若`H(x) = F(x) + λx`：
- 当λ > 1时，很有可能发生**梯度爆炸**；
- 当λ < 1时，梯度变成0，发生**梯度消失**，会阻碍残差网络信息的反向传递，从而影响残差网络的训练。
- 同理，其他常见的激活函数都会产生和上面的例子类似的阻碍信息反向传播的问题。
- 对于其它不影响梯度的 ，例如LSTM中的门机制或者Dropout以及1 × 1中用于降维的卷积，依旧是直接映射的效果最好

### 激活函数的位置
激活函数在残差网络中的使用：

<div align=center>
<img src="https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/激活函数在残差网络中的使用.jpg"/>
</div>

结果显示：将激活函数移动到残差部分（对应图e）可以提高模型的精度：

<div align=center>
<img src="https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/基于激活函数位置的变异模型在Cifar10上的实验结果.jpg"/>
</div>

### 残差网络与模型集成
残差网络可以从模型集成的角度理解。对于一个3层的残差网络可以展开成一棵含有8个节点的二叉树，而最终的输出便是这8个节点的集成。
- 随机删除残差网络的一些节点网络的性能变化较为平滑
- 对于VGG等stack到一起的网络来说，随机删除一些节点后，网络的输出将完全随机。

<div align=center>
<img src="https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/残差网络展开成二叉树.jpg"/>
</div>

