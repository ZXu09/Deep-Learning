## 残差网络
### 梯度消失问题
反向传播更新参数时用的是链式法则，在很深的网络层中，由于参数初始化一般更靠近0，这样在训练的过程中更新浅层网络的参数时，很容易使得梯度就接近于0，而导致梯度消失，浅层的参数无法更新。

### 网络退化问题
假设已经有了一个最优化的网络结构，是18层，假设设计了多于18层的网络模型，往往模型很难将这多余层恒等映射的参数学习正确，那么就一定会不比最优化的18层网络结构性能好，这就是随着网络深度增加，模型会产生退化现象。它**不是由过拟合产生的**，而是由**冗余的网络层**学习了**不是恒等映射的参数**造成的。

1. 网络发生退化：随着网络层数的增加，网络发生了**退化（degradation）** 的现象：随着网络层数的增多，训练集loss逐渐下降，然后趋于饱和，当你再增加网络深度的话，**训练集loss反而会增大**。注意这并不是过拟合，因为在过拟合中训练loss是一直减小的。
2. 特征传递：当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把**低层的特征传到高层**，那么效果应该至少不比浅层的网络效果差。
3. 具体方法：直接映射。或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的**特征**，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条**直接映射（Identity Mapping）** 来达到此效果。
4. 残差网络：保证信息不丢失。前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的**直接映射**的加入，保证了 l+1 层的网络一定比 l 层包含**更多的图像信息**。基于这种使用**直接映射**来连接网络不同层直接的思想，**残差网络**应运而生。
![残差块](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E6%AE%8B%E5%B7%AE%E5%9D%97.png)
