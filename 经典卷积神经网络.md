## CV常见名词
- Concat: 特征图按照通道维度直接进行拼接，例如8 * 8 * 16的特征图与8 * 8 * 16的特征图拼接后生成8 * 8 * 32的特征图  
注意：残差网络是直接相加
- 上采样层(upsample)：作用是将小尺寸特征图通过插值等方法，生成大尺寸图像
- 准确率(Precision)：预测之中的正确率（挑出的西瓜中好瓜的概率）
- 召回率(Recall)：好瓜有多少比例被挑出来了
- AP(average precision)：当交并比大于某一阈值(通常是0.5)，则认为正确。对每个类别，画出它的查准率-查全率曲线，**平均准确率是曲线下的面积**
- mAP(mean average precision)：**对所有类别的AP求平均值**，即为均值平均准确率
- mAP-50：按照IOU=0.5算出来的mAP-50。按照COCO标准算出来的mAP，**COCO要计算IOU从0.5到0.95之间间隔为0.05的10个平均值。
- IOU：(A∩B)/(A∪B)
- GIOU：IOU - (C-A∪B)/C，C对应AB的闭包框

### 机器学习和深度学习的差别联系
深度学习是机器学习中的一种方法，它使用了深度神经网络（Deep Neural Networks）来学习数据的特征和规律。  
而机器学习则更加广泛，它包括许多不同的算法和技术，如监督学习、无监督学习、半监督学习、强化学习等等。

## 梯度下降法
### 批量梯度下降法
如果使用梯度下降法(批量梯度下降法)，那么每次迭代过程中都要对**n个样本**进行**求梯度**，所以开销非常大。

### 随机梯度下降法（stochastic gradient descent，SGD）
随机梯度下降的思想就是随机采样**一个样本**来更新参数  
随机梯度下降虽然提高了计算效率，降低了计算开销，但是由于每次迭代只随机选择一个样本，因此随机性比较大，所以下降过程中非常曲折。

### 小批量梯度下降法
可以选取一定数目的样本组成一个**小批量样本**，然后用这个小批量更新梯度  
lr_decay_type 学习率下降：cos  
weight_decay：权值衰减，可防止过拟合  

### 自适应矩估计（Adaptive Moment Estimation，Adam）
SGD 低效的根本原因是，梯度的方向并没有指向最小值的方向。为了改正SGD的缺点，引入了Adam。  
梯度下降速度快，但是容易在最优值附近震荡。

## 分类
### Softmax
输出一个K维实向量σ(z)；  
这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取**概率最大**（也就是值对应最大的）结点，作为我们的预测目标。

### Logistic regression
实现多标签分类就需要用Logistic regression分类器来对每个类别都进行二分类。  
要用到了sigmoid函数，它可以把输出约束在0到1，如果某一特征图的输出经过该函数处理后的值大于设定阈值，那么就认定该目标框所对应的目标属于该类。

## 卷积组件
### 卷积层
**从输入中提取有用的特征**
- 卷积操作的本质是处理一个像素点和周围像素点的关系。
- 卷积是元素的乘法和加法。

### 1×1卷积
- 有效减少维度
- 有效低维嵌入，把这些通道嵌入到单个通道中
- 卷积后再应用非线性，在1×1卷积之后，可以添加非线性激活，如relu。非线性允许网络学习更复杂的功能。

## 卷积过程
每一个卷积核的**通道数量**，必须要求与输入**通道数量**一致，因为每个卷积核的对应**通道**与输入的对应**通道**进行卷积运算

![卷积过程](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png)

如果要卷积后也输出多通道，增加卷积核（filers）的数量即可，相当于不同权重的卷积核去计算，示意图如下：

![输出多通道卷积](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/%E8%BE%93%E5%87%BA%E5%A4%9A%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF.png)

### 1×1卷积先降维再升维的特点：

第一个1x1的卷积把256维channel降到64维（64个卷积核），然后在最后通过1x1卷积恢复，参数数目：1x1x256x64（每个卷积核1x1x256，共计64个） + 3x3x64x64 + 1x1x64x256 = 69632，

全连接层(左图)就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。

![1×1卷积](https://github.com/SZUZOUXu/Deep-Learning/blob/main/image/1%C3%971%E5%8D%B7%E7%A7%AF.png)

### 池化层
最主要作用就是**压缩图像**。
1. **减少参数**，预防网络过拟合。
2. 保持**特征不变性**，保留特征，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)

### 全连接层(fully connected layers,FC)
卷积取的是**局部特征**，全连接就是把以前的**局部特征重新通过权值矩阵组装成完整的图**。因为用到了所有的局部特征，所有叫全连接。在整个卷积神经网络中起到**分类器**的作用。

### 为什么采用梯度下降
深度网络是由许多非线性层(带有激活函数)堆叠而成，每一层非线性层可以视为一个非线性函数 f(x) ，因此整个深度网络可以视为一个复合的非线性多元函数。  
优化深度网络就是为了寻找到合适的权值，满足损失取得最小值点，比如简单的损失函数平方差(MSE)  
在数学中寻找最小值问题，采用梯度下降的方法再合适不过了。

### 过拟合解决办法
1. 增加数据量，数据增强
2. 正则化
- Dropout：（正规化方法）  
专门用在神经网络的正规化的方法, 叫作 dropout. 在训练的时候, 我们随机忽略掉一些神经元和神经联结 , 是这个神经网络变得”不完整”. 用一个不完整的神经网络训练一次.
- BN
- L1正则化
L1正则化是所有参数的绝对值之和，这就要求所有参数的绝对值之和最小，有可能导致某些参数在更新的时候，值趋近于0。
- L2正则化
数学公式上是所有参数平方和，对其求导是2w，从这个可以看出，每个参数更新的量其实是和参数w本身相关的，w在更新时，会逐渐变慢，所以对w本身来说，都会逐渐变小，而不会变为0。
3. 减少参数量
4. 权值衰减

## 正则化
### BatchNormalization
**BatchNormalization（批量标准化）** 是一种常见的正则化技术。它的主要作用是**规范化输入数据**，使其在神经网络中**更容易传播**，从而**提高训练效果**。  
通过减去均值并除以标准差使得输入数据~N(0,1)
1. **加速模型的训练**：通过减少输入数据分布的变化，可以使模型**更快地收敛**。
2. **减轻过拟合，增强模型的泛化能力**：参数更新依赖于整个batch size的数据，一定程度上防止了过拟合，增强模型的泛化能力
3. **提高网络训练稳定性，增加模型的深度**：由于BatchNormalization可以使得**输入数据分布更加稳定**，因此可以使得模型更容易训练，从而允许更深的网络结构。

### 标准化的方法
Layer Normalization、Instance Normalization、Group Normalization、 Weight Normalization。

### Dropout
使用dropout可以**使得部分节点失活**，可以起到**简化神经网络结构**的作用，从而起到正则化的作用。  
因为dropout是使得神经网络的**节点随机失活**，这样会让神经网络在训练的时候不会使得某一个节点权重过大。因为该节点输入的特征可能会被清除，所以神经网络的节点不能依赖任何输入的特征。dropout最终会产生收缩权重的平方范数的效果，来压缩权重，达到类似于L2正则化的效果。

## 激活函数
激活函数的作用是：**增加模型的非线性表达能力**    
一个神经网络由层节点组成，并学习将输入的样本映射到输出。对于给定的节点，将**输入乘以节点中的权重，并将其相加**。此值称为节点的summed activation。然后，经过求和的激活通过一个激活函数转换并定义特定的输出或节点的“activation”。

最简单的激活函数被称为**线性激活**，其中根本没有应用任何转换。 一个仅由线性激活函数组成的网络很容易训练，但**不能学习复杂的映射函数**。线性激活函数仍然用于预测一个数量的网络的输出层(例如回归问题)。

**非线性激活函数**是更好的，因为它们允许**节点在数据中学习更复杂的结构** 。两个广泛使用的非线性激活函数是sigmoid 函数和双曲正切激活函数。
### sigmoid 函数
Logistic函数神经网络，传统上是一个非常受欢迎的神经网络激活函数。函数的输入被转换成介于0.0和1.0之间的值。大于1.0的输入被转换为值1.0，同样，小于0.0的值被折断为0.0。所有可能的输入函数的形状都是从0到0.5到1.0的 s 形。  
公式为：S(x) = 1 / (1 + e^-x)图像类似于S型

### tanh 函数
形状类似sigmoid 函数的非线性激活函数，输出值介于-1.0和1.0之间。  
Sigmoid和 tanh 函数的一个普遍问题是它们**值域饱和**了 。这意味着，大值突然变为1.0，小值突然变为 -1或0。此外，函数只对其输入**中间点周围的变化非常敏感**。

###  ReLU（Rectified Linear Activation Function）函数
**极简方式实现非线性激活，缓解了梯度消失**
ReLU函数可以将所有小于0的输入值设置为0，而将所有大于0的输入值保持不变。
```
if input > 0:
  return input
else:
  return 0
```
1. **优化神经网络的训练速度**：由于ReLU函数的**梯度在输入大于0的区域恒为1**，因此反向传播时**不会出现梯度消失**问题，从而使得神经网络的训练速度更快。
2. **降低模型的过拟合，提高模型的泛化能力**：ReLU可以通过**对输入值进行截断**，减少模型**对噪声的敏感度**，从而降低模型的过拟合风险；减少参数的数量，提高模型的泛化能力。
3. **线性的优点**：对于大于零的值，这个函数是线性的，这意味着当使用反向传播训练神经网络时，它具有很多**线性激活函数的理想特性**。
4. **非线性的优点**，它是一个非线性函数，因为负值总是作为零输出。使得模型可以更好地拟合非线性数据，**允许学习数据中的复杂关系**。

### Dead Relu：  
在训练过程中，由于一次**梯度更新的幅度过大**，导致某些Relu节点的**权重调整的太大**，使得**后续的训练对该节点不再起作用**，这个节点相当于永久dead了。  
Relu的输入值为负的时候，输出始终为0，其一阶导数也始终为0，这样会导致神经元不能更新参数。

### Leaky ReLU：  
普通的ReLU是将所有的负值都设为零，其一阶导数也始终为0，这样会导致神经元不能更新参数。（Dead ReLU）Leaky ReLU则是给所有负值赋予一个非零斜率。
**Leaky ReLU**解决了**Relu函数进入负区间后，导致神经元不学习的问题。  

### 正向传播、反向传播
1. 前向传播：将训练集数据输入经过隐藏层，最后到达输出层并输出结果。【输入层—隐藏层–输出层】
2. 反向传播：由于输入结果与输出结果有误差，则计算**估计值与实际值之间的误差（loss）**，并将该误差从输出层向隐藏层反向传播，直至传播到输入层。【输出层–隐藏层–输入层】
3. 权重更新：在反向传播的过程中，根据**误差调整各种参数的值**；不断迭代上述过程，直至收敛。
以逻辑回归的神经元为例：

$$
z = w_1x_1+w_2x_2 + b
$$

带入激活函数a = f(z)中，记损失函数为l，反向传播的最终目的是**修正权值w**，那么我们让**l(loss)对w(参数)求偏导(梯度)**：

$$
\frac{\partial l}{\partial w} = \frac{\partial l}{\partial z}\frac{\partial z}{\partial w}
$$

右半部分很好求，属于**前向传播**得到的结果：

$$
\frac{\partial z}{\partial w_1} = x1，
\frac{\partial z}{\partial w_2} = x2
$$

左半部分属于**后向传播**的过程：

$$
\frac{\partial l}{\partial z} = \frac{\partial l}{\partial a}\frac{\partial a}{\partial z} = \frac{\partial l}{\partial a}f^{\prime}(x)
$$

因此需要**反向传播损失函数l对激活函数a=f(z)进行求导**

### 梯度消失问题
如果此部分小于1，那么随着层数的增加求出的梯度更新的信息会以指数形式衰减，如果某一次等于0，那么相乘就全为0这就是梯度消失。

### 梯度爆炸问题
在反向传播对激活函数进行求导的时候，如果此部分大于1，那么随着层数的增加，求出的梯度的更新将以指数形式增加，发生梯度爆炸。

### 梯度消失、爆炸的解决方案
1. 预训练和微调  
预训练：无监督逐层训练，每次训练一层隐藏点，训练时将**上一层隐节点的输出作为输入**，而本层隐节点的输出作为下一层隐节点的输入。称为逐层预训练。在预训练完成后还要对整个网络进行微调。
2. 梯度剪切、正则  
梯度剪切又叫梯度截断，是防止**梯度爆炸**的一种方法，其思想是设置一个梯度剪切阈值，更新梯度的时候，如果梯度超过这个阈值，那就将其强制限制在这个范围之内。
3. relu、leakyrelu、elu等激活函数  
从relu的函数特性我们知道，在小于0的时候梯度为0，大于0的时候梯度恒为1，那么此时就不会再存在梯度消失和梯度爆炸的问题了，因为每层的网络得到的梯度更新速度都一样。
4. Batch Normalization(批规范化)  
BatchNormalization可以使得**输入数据分布更加稳定**
5. 残差结构  
我们可以抽象的理解为残差网络就是把里面的**连乘变成了连加**，这样就解决了**某一个权值求导为0**的情况的影响，即：**局部不会影响全局**。
6. LSTM（长短期记忆网络）  
在RNN（循环神经网络）网络结构中，由于使用sigmoid或者Tanh函数，所以很容易导致梯度消失的问题，即在相隔很远的时刻时，前者对后者的影响几乎不存在了，**LSTM的机制**正是为了解决这种长期依赖问题。

### 网络退化问题
假设已经有了一个最优化的网络结构，是18层，假设设计了多于18层的网络模型，往往模型很难将这多余层恒等映射的参数学习正确，那么就一定会不比最优化的18层网络结构性能好，这就是随着网络深度增加，模型会产生退化现象。  
它**不是由过拟合产生的**，而是由**冗余的网络层**学习了**不是恒等映射的参数**造成的。

